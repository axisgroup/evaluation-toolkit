## Current Evaluation Scenario Pitfalls
1. **Presenting your design is not the same as evaluating your design**

   While the former requires the mindset of defending design choices (to some degree), the latter requires a very unbiased and open mind to critique.
![Valudate vs. Test](./Assets/images/cupcake-mvp.png)

2. **Not all feedback is equal**

   Open-ended questions such as “What do you think about this design?”, while sometimes useful, can yield a huge variety of feedback that is oftentimes unfocused and difficult to sort through. This poses a serious challenge especially in light of the limited time we have to not only gather feedback but also to make changes based on the feedback. So, it will be important to set out with a specific goal and strategy in mind, particularly to prevent interviewees from providing feedback just for the sake of feedback.  

3. **One hour with 5+ people exacerbates this problem**

   Group evaluation methods, while acceptable, should be heavily moderated so that the evaluation process is democratic and not overtaken by the strongest and most dominant voice in the room. Failing to do so leaves room for existing hierarchical structures to interfere with the evaluation process, and it could lead to a high-status stakeholder overpowering an end user’s feedback.

4. **Implementing feedback is a collective task**

    The task of implementing feedback should be borne collectively by the both the designer and the stakeholders, in order to produce a solution that best meets the need of the end user. By leaving the floor open and placing the decision-making power solely on stakeholders, we expose ourselves to the risk of implementing opinions that do not align with the voice of the end user.

5. **Providing a link to InVision is not a proxy**

   Oftentimes, a link to InVision is in effect the same as leaving the floor wide open for unfocused and varied feedback. But more often than not, we find there to be a lack of incentive to leave feedback, perhaps due to a lack of guidance,  scaffolding, and understanding of what's open to critique. Some users face a lack confidence in how valuable their feedback is-- thus, we need to provide prompts in order to make this process more rewarding and effective to both parties.

## Where does this leave us?

Tons of techniques to uncover all kinds of feedback. But our resources are limited. As such, to reduce the risk of redesign we have to distribute the process of evaluation and quality control.

The Solution—

> **Heuristic techniques** often called simply a heuristic. It is any approach to problem solving, learning, or discovery that employs a practical method not guaranteed to be optimal or perfect, but sufficient for the immediate goals. 
>
> Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.Heuristics can be mental shortcuts that ease the cognitive load of making a decision. 
>
> Examples of this method include using a rule of thumb, an educated guess, an intuitive judgment, guesstimate, stereotyping, profiling, or common sense.

 [Source: Wikipedia](https://en.wikipedia.org/wiki/Heuristic)

Before we begin defining a testing approach, using a heuristic approach, it is important to identify a testing mindset that is compatible with our heuristic approach. This [Testing Mindset](TestingMindset.md) document helps align expectations, in accordance with expected outcomes.

## References:
- https://www.nngroup.com/articles/no-validate-in-ux/
- https://articles.uie.com/usability_testing_mistakes/
