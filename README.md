# Evaluation Toolkit
This repo is a collection of documents intended to help guide the testing process in a data visualization context. It contains guidelines that should not be followed exactly. It is written by and for the Axis Design team although it can also act as a point of reference for data vis practitioners.

## Know your guidelines before you break them

The goal of this repo is to serve as a **toolkit** that helps deconstruct the testing process into manageable chunks.
This toolkit aims to:

- help identify pitfalls in our current evaluation process, so designers can be more cognizant of them and control for them when possible.
- provide enough guidelines to designers to reduce the cognitive load of decision making at every stage.

Specifically, the approach advocated by this repo is three-pronged:

```
   	1. Follow best practices, don't re-invent the wheel
    	2. Know what to measure?
     	3. Know how to measure?
```

With these guidelines designers can determine success criteria for their project, prioritize what to test versus what not to test, and identify shortcut methods to test for them. 

Where possible, this toolkit will also attempt to identify tactics designers can employ to improve their evaluation process.  
This is a **WIP** living document. Please read our [Contribution Guidelines](CONTRIBUTING.md) to help refine this repo.

## How to use this toolkit

1. **Setting the Stage**: Before beginning the tests, it will be helpful to 
   - Understand the [pitfalls](./0.Setting-the-Stage/Pitfalls.md) with the current evaluation methodologies, in order to prevent yourself from falling into them, and
   - Equip yourself with the [testing mindset](./0.Setting-the-Stage/TestingMindset.md) in order to properly set expectations.
2. **Follow best practices:** This serves as an initial self-directed evaluation, intended to help you determine whether your visualization is following best practices, particularly in regard to styling. Reference the [data visualization checklist](./Assets/DataVizChecklist_May2016.pdf) prepared by Stephanie Evergreen and Ann K. Emery to evaluate specific UI elements such as typography, color, and arrangement.
3. **Know what to measure:** Next, you can begin the process of user testing by first determining what you speficially want to test and measure, and what attributes you want to prioritize for your dashboard. Do you want to test its usability, usefulness, desirability, or a combination of these attributes? Reference the [Determining what to measure](./2.Determining-what-to-measure/README.md) section for a list of questions that can be asked to help you determine whether your visualization achieves your prioritized attributes.
4. **Know how to measure:** Now that you have pinpointed the relevant questions and attributes, you can use reference this page to devise a testing plan and methodology.


Eventually, the goal is to be able to fill out something like this [excel doc](https://docs.google.com/spreadsheets/d/1lfcPwG4gH-rQQhl5MuXgNevy8_hlJPvdx6_RiLT34qw/edit#gid=0) that tracks how research questions mature as the design sprint progresses and what methods were used to answer those questions.

Sample filled out doc-

![Testing Template](./Assets/images/Sample-Testing-Template.png)

Attribution: [Arielle Cason, UX Researcher](http://ariellecason.com/)

