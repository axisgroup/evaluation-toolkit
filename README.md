## Why we test?
As designers we are often confronted with the question “How do we know our design works”?, whether it's a matter of introspection or presenting and justifying our design decisions.

And while testing seems like the obvious answer, it is important to realize that testing is not a single summative phase in the entire project (as in SDLC), but rather a formative process that is distributed throughout the design process.

In fact, testing is one of the fundamental pillars of the User-Centered Design Sprint process at Axis which is based on:

- Empathy

- Creativity
- Testing

Using these fundamentals we are able to iteratively deliver applications that are "Useful, Usable and Delightful" at every stage.

![Cupcake Analogy](./Assets/images/cupcake_mvp.png)



## But how do REALLY test during a sprint?

Because testing is so distributed, it can manifest in many ways ranging from casual coffee shop studies to  controlled experiments depending on the research context, time at hand, access to participants etc.

And while testing is a core philosophy of the UCD process, a point of tension emerges during **Lean Design Sprints**, where time and resources are limited and compromises must be made in the name of fast delivery.

> How then can Designers balance the need for robust testing vs. quick turnaround times?



# Heuristic approach

> **Heuristic techniques** often called simply a heuristic. It is any approach to problem solving, learning, or discovery that employs a practical method not guaranteed to be optimal or perfect, but sufficient for the immediate goals. 
>
> Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.Heuristics can be mental shortcuts that ease the cognitive load of making a decision. 
>
> Examples of this method include using a rule of thumb, an educated guess, an intuitive judgment, guesstimate, stereotyping, profiling, or common sense.

 [Source: Wikipedia



## Know your guidelines before you break them

The goal of this repository is to serve as a **toolkit**-

- to help identify [pitfalls](Pitfalls.md) in our current evaluation process so designers can be more cognizant of them and control for them when possible.
- to provide enough guidelines to designers to reduce the cognitive load of decision making at every stage.

This repository will also attempt to identify tactics designers can employ to improve their evaluation process. 

Specifically, the approach advocated  by this repository is three-pronged:

 	1. Follow best practices, Don't re-invent the wheel
 	2. Know what to measure?
 	3. Know how to measure?

— so designers can determine success criteria for their project, prioritize what to test vs. what not to and identify shortcut mehtods to test for them.

This is a **WIP** living document; Please read our [Contribution Guidelines](CONTRIBUTION.md) to help refine this repository.



